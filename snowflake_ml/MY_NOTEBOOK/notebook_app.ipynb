{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "f3984623-933f-4ab6-a6fa-878fb6f3c4b3",
   "metadata": {
    "language": "python",
    "name": "cell8",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "session = get_active_session()\ndf = session.sql(\"SELECT * FROM MEDICARE.PUBLIC.MEDICARE_TABLE LIMIT 1000\")\ndf.collect()\npandas_df = df.to_pandas()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2be95b17-a8a1-48c4-88e8-963089e34325",
   "metadata": {
    "language": "python",
    "name": "cell5",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "\nfrom io import BytesIO\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nimport joblib\nimport json\nfrom azure.storage.blob import BlobServiceClient\nfrom snowflake.snowpark.files import SnowflakeFile\nimport snowflake.connector\nfrom snowflake.snowpark.context import get_active_session\n\ndef preprocess_data(input_data):\n\n    # Initialize Snowflake session\n\n    session = get_active_session()\n    # Load and preprocess data\n    model_file = \"@MEDICARE.PUBLIC.DATA/dataset1.csv\"\n    #df = pd.read_csv(SnowflakeFile.open(model_file, 'rb', require_scoped_url=False))\n    session = get_active_session()\n    df = session.sql(\"SELECT * FROM MEDICARE.PUBLIC.MEDICARE_TABLE LIMIT 1000\")\n    df.collect()\n    df =  df.to_pandas()\n\n    X = df.drop('V28HCCCODED', axis=1)\n    y = df['V28HCCCODED']\n    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n\n    numeric_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='mean')),\n        ('scaler', StandardScaler())\n    ])\n    categorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, numerical_cols),\n            ('cat', categorical_transformer, categorical_cols)\n        ])\n\n    # Apply transformations\n    X_transformed = preprocessor.fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_transformed, y, test_size=0.2, random_state=42, stratify=y)\n    print('Hii')\n\n    # Train model\n    model = LogisticRegression(max_iter=1000)\n    model.fit(X_train, y_train)\n    print('model trained')\n\n    # Save model to in-memory bytes buffer\n    local_file_path = './logistic_regression_model.pkl'\n    joblib.dump(model, local_file_path)\n    buffer = BytesIO()\n    joblib.dump(model, buffer)\n    buffer.seek(0)\n    stage_name = '@MEDICARE.PUBLIC.DATA'\n    put_result = session.file.put(local_file_path, \"@MEDICARE.PUBLIC.DATA/logistic_regression_model.pkl\")\n    #FileOperation.put(local_file_name = local_file_path,stage_location = stage_name)\n\n   \n\n    return put_result[0]\n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f127753e-a76f-4255-a109-639e74bc2e67",
   "metadata": {
    "language": "python",
    "name": "cell6",
    "collapsed": false,
    "resultHeight": 1037
   },
   "outputs": [],
   "source": "preprocess_data('Hi')",
   "execution_count": null
  }
 ]
}